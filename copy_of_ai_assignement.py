# -*- coding: utf-8 -*-
"""Copy of AI Assignement

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13MX80zL4Aw51O0lPAuWsb-43jj6SYkAN
"""

import fitz
import os

# Load the PDF
pdf_path = '2307.06435v9.pdf'
pdf_document = fitz.open(pdf_path)

# Create directories to save images that exist in the pdf
if not os.path.exists('extracted_images'):
    os.makedirs('extracted_images')

# Function to extract images and text from each page of the pdf
def extract_text_and_images(pdf_document):
    text_chunks = []
    images = []

    for page_num in range(len(pdf_document)):
        page = pdf_document.load_page(page_num)

        # Extract text
        text = page.get_text("text")
        text_chunks.append(text)

        # Extract images
        image_list = page.get_images(full=True)
        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = pdf_document.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            image_name = f"page_{page_num+1}_img_{img_index}.{image_ext}"
            image_path = os.path.join('extracted_images', image_name)
            with open(image_path, "wb") as img_file:
                img_file.write(image_bytes)
            images.append(image_path)

    return text_chunks, images

# Run the extraction
text_chunks, images = extract_text_and_images(pdf_document)

import re

# Function to split text into chunks
def split_text_into_chunks(text_chunks, max_chunk_size=1000):
    split_chunks = []

    for text in text_chunks:
        # Split the split text by sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)

        chunk = ""
        for sentence in sentences:
            if len(chunk) + len(sentence) <= max_chunk_size:
                chunk += sentence + " "
            else:
                split_chunks.append(chunk.strip())
                chunk = sentence + " "

        # Add the last chunk if it's not empty
        if chunk:
            split_chunks.append(chunk.strip())

    return split_chunks

# Split the text into chunks
text_chunks_split = split_text_into_chunks(text_chunks)

# Load a pre-trained model for text embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for text chunks
text_embeddings = model.encode(text_chunks_split, convert_to_tensor=True)

# Displaying the shape of the text embeddings
print("Shape of text embeddings:", text_embeddings.shape)

from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import torch


# Load a pre-trained CLIP model
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Generate embeddings for images
image_embeddings = []

for image_path in images:
    image = Image.open(image_path)
    inputs = clip_processor(images=image, return_tensors="pt")
    embeddings = clip_model.get_image_features(**inputs)
    image_embeddings.append(embeddings)

# Convert the list of tensors to a single tensor
image_embeddings = torch.cat(image_embeddings)

# Displaying the shape of the image embeddings
print("Shape of image embeddings:", image_embeddings.shape)

import torch
import gc  # Garbage collector
import numpy as np
from transformers import CLIPProcessor, CLIPModel
from sentence_transformers import SentenceTransformer
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity

# Load the models
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
text_model = SentenceTransformer('all-MiniLM-L6-v2')

# Batch processing function for text
def generate_text_embeddings_in_batches(texts, batch_size=8):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        inputs = clip_processor(text=batch, return_tensors="pt", padding=True, truncation=True, max_length=77)
        with torch.no_grad():
            batch_embeddings = clip_model.get_text_features(**inputs).cpu()
        embeddings.append(batch_embeddings)
        del batch_embeddings
        gc.collect()  # Clear memory
    return torch.cat(embeddings)

# Batch processing function for images
def generate_image_embeddings_in_batches(images, batch_size=8):
    embeddings = []
    for i in range(0, len(images), batch_size):
        batch = images[i:i+batch_size]
        batch_embeddings = []
        for image_path in batch:
            image = Image.open(image_path)
            inputs = clip_processor(images=image, return_tensors="pt")
            with torch.no_grad():
                embedding = clip_model.get_image_features(**inputs).cpu()
            batch_embeddings.append(embedding)
        embeddings.append(torch.cat(batch_embeddings))
        del batch_embeddings
        gc.collect()  # Clear memory
    return torch.cat(embeddings)

# Function to filter significant text chunks
def filter_significant_text_chunks(text_chunks, min_length=30):
    """
    Filters out text chunks that are empty or too short.
    """
    filtered_chunks = [chunk for chunk in text_chunks if len(chunk.strip()) >= min_length]
    return filtered_chunks

# Debug function to identify empty or insignificant text chunks
def debug_empty_chunks(text_chunks):
    for i, chunk in enumerate(text_chunks):
        if len(chunk.strip()) == 0:
            print(f"Empty chunk found at index {i}")
        elif len(chunk.strip()) < 30:
            print(f"Insignificant chunk found at index {i}: '{chunk.strip()}'")

# The search function
def search(query, text_embeddings, image_embeddings, text_chunks, images):
    query_embedding = generate_text_embeddings_in_batches([query], batch_size=1)
    text_similarities = cosine_similarity(query_embedding.numpy(), text_embeddings.numpy())
    text_similarities = text_similarities.flatten()

    image_similarities = cosine_similarity(query_embedding.numpy(), image_embeddings.numpy())
    image_similarities = image_similarities.flatten()

    top_text_indices = np.argsort(-text_similarities)[:3]
    top_text_chunks = [(text_chunks[i], text_similarities[i]) for i in top_text_indices]

    top_image_indices = np.argsort(-image_similarities)[:3]
    top_images = [(images[i], image_similarities[i]) for i in top_image_indices]

    return top_text_chunks, top_images

# Apply the filtering
filtered_text_chunks = filter_significant_text_chunks(text_chunks_split)

# Debug the filtered chunks
debug_empty_chunks(filtered_text_chunks)

# Generate embeddings in batches
text_embeddings_clip = generate_text_embeddings_in_batches(filtered_text_chunks)
image_embeddings_clip = generate_image_embeddings_in_batches(images)

# Perform the search with the filtered text chunks
top_text_chunks, top_images = search(query, text_embeddings_clip, image_embeddings_clip, filtered_text_chunks, images)

# Example query
query = "fine tuning"

# Display the results
print("Top 3 Text Chunks:")
for chunk, score in top_text_chunks:
    print(f"Score: {score:.4f} - Text: {chunk}\n")

print("\nTop 3 Images:")
for img_path, score in top_images:
    print(f"Score: {score:.4f} - Image Path: {img_path}")

"""The RAG completed"""

# Load a pre-trained generative model ( BART in this case)
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")

def generate_answer(query, retrieved_chunks):
    # Concatenate the retrieved chunks into a single context
    context = " ".join([chunk for chunk, _ in retrieved_chunks])

    # Encode the input (query + context)
    inputs = tokenizer(query + " " + context, return_tensors="pt", max_length=1024, truncation=True)

    # Generate a response
    outputs = model.generate(inputs.input_ids, max_length=150, num_beams=5, early_stopping=True)

    # Decode the response
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return answer

# Example usage with retrieved chunks
top_text_chunks, _ = search(query, text_embeddings_clip, image_embeddings_clip, filtered_text_chunks, images)
answer = generate_answer(query, top_text_chunks)

print("Here is the generated Answer:", answer)

